{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPQSOWxMkxU8rkVC/8Quo/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quarup/afterlife-ai/blob/main/afterlife_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# end-to-end"
      ],
      "metadata": {
        "id": "fXsXLCuVbk7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install packages\n",
        "!pip install openai-whisper gradio transformers accelerate bitsandbytes TTS"
      ],
      "metadata": {
        "id": "oDL_z7VSbo5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y espeak-ng"
      ],
      "metadata": {
        "id": "Yb1OWdwCLxvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title imports\n",
        "import whisper\n",
        "import gradio as gr\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import os\n",
        "import numpy as np\n",
        "from TTS.api import TTS\n",
        "import soundfile as sf\n",
        "from google.colab import drive\n",
        "import datetime"
      ],
      "metadata": {
        "id": "Cif0Tu03bsk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title set up directories\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories on Google Drive\n",
        "drive_base_dir = \"/content/drive/MyDrive/AI_Chat_App\"\n",
        "voice_samples_dir = os.path.join(drive_base_dir, \"voice_samples\")\n",
        "responses_dir = os.path.join(drive_base_dir, \"ai_responses\")\n",
        "conversations_dir = os.path.join(drive_base_dir, \"conversations\")\n",
        "\n",
        "for directory in [drive_base_dir, voice_samples_dir, responses_dir, conversations_dir]:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "\n",
        "print(f\"Using Google Drive directories at: {drive_base_dir}\")"
      ],
      "metadata": {
        "id": "RcxlyWg8cWPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load models\n",
        "# Load Whisper model\n",
        "print(\"Loading Whisper model...\")\n",
        "whisper_model = whisper.load_model(\"base\")  # You can use \"tiny\", \"base\", \"small\", \"medium\", or \"large\"\n",
        "print(\"Whisper model loaded!\")\n",
        "\n",
        "# Load DeepSeek model with quantization to reduce memory usage\n",
        "print(\"Loading DeepSeek model...\")\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
        "\n",
        "# Quantization configuration for memory efficiency\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "print(\"DeepSeek model loaded!\")\n"
      ],
      "metadata": {
        "id": "EQiyiZG4ccwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TTS models\n",
        "\n",
        "# Load language-specific high-quality voice cloning models\n",
        "print(\"Loading language-specific voice cloning models...\")\n",
        "\n",
        "# Dictionary to hold our language-specific models\n",
        "tts_models = {}\n",
        "\n",
        "# Try to load models with better error handling\n",
        "try:\n",
        "    # First try a reliable English model\n",
        "    tts_models[\"en\"] = TTS(\"tts_models/en/ljspeech/tacotron2-DDC\")\n",
        "    print(\"Loaded English TTS model\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading English model: {str(e)}\")\n",
        "    # We'll use the fallback model for English\n",
        "\n",
        "# try:\n",
        "#     # For Portuguese\n",
        "#     tts_models[\"pt\"] = TTS(\"tts_models/pt/cv/vits\")\n",
        "#     print(\"Loaded Portuguese TTS model\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error loading Portuguese model: {str(e)}\")\n",
        "#     # We'll use the fallback model for Portuguese\n",
        "\n",
        "# try:\n",
        "#     # For German\n",
        "#     tts_models[\"de\"] = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\n",
        "#     print(\"Loaded German TTS model\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error loading German model: {str(e)}\")\n",
        "#     # We'll use the fallback model for German\n",
        "\n",
        "# Load language-specific high-quality voice cloning models\n",
        "print(\"Loading English-focused voice cloning model...\")\n",
        "\n",
        "try:\n",
        "    # YourTTS is better at maintaining English speech patterns during cloning\n",
        "    english_clone_model = TTS(\"tts_models/multilingual/multi-dataset/your_tts\")\n",
        "    print(\"Loaded YourTTS model for English voice cloning\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YourTTS model: {str(e)}\")\n",
        "    print(\"Will fall back to XTTS-v2 for English voice cloning\")\n",
        "    english_clone_model = None\n",
        "\n",
        "# Load the XTTS-v2 model which we know works for voice cloning\n",
        "print(\"Loading XTTS-v2 model for voice cloning...\")\n",
        "fallback_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
        "print(\"XTTS-v2 model loaded for voice cloning!\")\n",
        "\n",
        "# Get available models to ensure we're using valid ones\n",
        "print(\"\\nListing available TTS models for reference:\")\n",
        "print(TTS().list_models())"
      ],
      "metadata": {
        "id": "jt8OIRphMD94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run chat\n",
        "# Variables to store user voice sample\n",
        "user_voice_sample = os.path.join(voice_samples_dir, \"default_voice.wav\")\n",
        "current_language = \"en\"\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"Transcribe audio using Whisper\"\"\"\n",
        "    # Whisper expects the audio file path\n",
        "    result = whisper_model.transcribe(audio_file)\n",
        "    return result[\"text\"]\n",
        "\n",
        "def text_to_speech(text, voice_sample=None):\n",
        "    \"\"\"\n",
        "    Enhanced TTS function that uses:\n",
        "    1. Language-specific model for English when no voice cloning is needed\n",
        "    2. YourTTS for English voice cloning (better at staying in English)\n",
        "    3. fallback_model for Portuguese and German (all cases)\n",
        "    4. fallback_model as final fallback for all cases if other options fail\n",
        "    \"\"\"\n",
        "    global current_language\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = os.path.join(responses_dir, f\"response_{current_language}_{timestamp}.wav\")\n",
        "\n",
        "    # Preprocess text for the specific language\n",
        "    processed_text = preprocess_text_for_language(text, current_language)\n",
        "\n",
        "    try:\n",
        "        # CASE 1: English without voice cloning - use dedicated English model if available\n",
        "        if current_language == \"en\" and voice_sample is None and \"en\" in tts_models:\n",
        "            tts_models[\"en\"].tts_to_file(\n",
        "                text=processed_text,\n",
        "                file_path=output_path\n",
        "            )\n",
        "            print(f\"Used dedicated English model without voice cloning\")\n",
        "            return output_path\n",
        "\n",
        "        # CASE 2: English WITH voice cloning - use YourTTS if available (better English cloning)\n",
        "        if current_language == \"en\" and voice_sample is not None and english_clone_model is not None:\n",
        "            english_clone_model.tts_to_file(\n",
        "                text=processed_text,\n",
        "                file_path=output_path,\n",
        "                speaker_wav=voice_sample,\n",
        "                language=current_language\n",
        "            )\n",
        "            print(f\"Used YourTTS for English voice cloning\")\n",
        "            return output_path\n",
        "\n",
        "        # CASE 3: All other cases - use fallback_model\n",
        "        # This includes:\n",
        "        # - Portuguese and German (with or without voice cloning)\n",
        "        # - English with voice cloning if YourTTS failed or isn't available\n",
        "        # - English if dedicated model isn't available\n",
        "\n",
        "        # With voice cloning\n",
        "        if voice_sample is not None:\n",
        "            fallback_model.tts_to_file(\n",
        "                text=processed_text,\n",
        "                file_path=output_path,\n",
        "                speaker_wav=voice_sample,\n",
        "                language=current_language\n",
        "            )\n",
        "            print(f\"Used fallback_model with voice cloning for {current_language}\")\n",
        "        # Without voice cloning\n",
        "        else:\n",
        "            fallback_model.tts_to_file(\n",
        "                text=processed_text,\n",
        "                file_path=output_path,\n",
        "                language=current_language\n",
        "            )\n",
        "            print(f\"Used fallback_model without voice cloning for {current_language}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in TTS processing: {str(e)}\")\n",
        "\n",
        "        # Last resort fallback - use fallback_model with minimal options\n",
        "        try:\n",
        "            fallback_model.tts_to_file(\n",
        "                text=processed_text,\n",
        "                file_path=output_path,\n",
        "                language=current_language\n",
        "            )\n",
        "            print(f\"Used emergency fallback for {current_language}\")\n",
        "            return output_path\n",
        "        except Exception as e2:\n",
        "            print(f\"All TTS methods failed: {str(e2)}\")\n",
        "            return None\n",
        "\n",
        "def preprocess_text_for_language(text, language_code):\n",
        "    \"\"\"Apply language-specific preprocessing to improve TTS quality\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Common preprocessing for all languages\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "\n",
        "    # Replace problematic characters\n",
        "    replacements = {\n",
        "        \"…\": \"...\",\n",
        "        \"–\": \"-\",\n",
        "        \"—\": \"-\",\n",
        "        \"\"\": '\"',\n",
        "        \"\"\": '\"',\n",
        "        \"'\": \"'\",\n",
        "        \"'\": \"'\",\n",
        "    }\n",
        "\n",
        "    for original, replacement in replacements.items():\n",
        "        text = text.replace(original, replacement)\n",
        "\n",
        "    # Language-specific preprocessing\n",
        "    if language_code == \"en\":\n",
        "        # For English models (SpeechT5/Tortoise/VITS)\n",
        "        # These models handle English text well, but we'll clean up non-English characters\n",
        "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    elif language_code == \"pt\":\n",
        "        # For Portuguese, preserve Portuguese-specific characters\n",
        "        # The PT-specific models need these characters for proper pronunciation\n",
        "        pass  # No extra processing needed for Portuguese\n",
        "\n",
        "    elif language_code == \"de\":\n",
        "        # For German, preserve German-specific characters\n",
        "        pass  # No extra processing needed for German\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_friend_name(language_code):\n",
        "    \"\"\"Returns the friend's name based on the language\"\"\"\n",
        "    if language_code == \"pt\":\n",
        "        return \"Quarup\"\n",
        "    else:  # en or de\n",
        "        return \"Q\"\n",
        "\n",
        "def generate_response(user_input, history):\n",
        "    \"\"\"\n",
        "    Generate response using DeepSeek model, ensuring it responds in the same language as the user\n",
        "    \"\"\"\n",
        "    global current_language\n",
        "\n",
        "    # Determine which language to use for the response\n",
        "    response_language = current_language\n",
        "\n",
        "    # Get the appropriate friend name for this language\n",
        "    friend_name = get_friend_name(response_language)\n",
        "\n",
        "    # Add a system instruction to encourage warmer responses in the appropriate language\n",
        "    # and include the appropriate name\n",
        "    if response_language == \"en\":\n",
        "        system_instruction = f\"You are Assistant {friend_name}, having a friendly, warm conversation in English. Respond in English using conversational language, express empathy, and occasionally use friendly phrases or light humor when appropriate.\"\n",
        "    elif response_language == \"pt\":\n",
        "        system_instruction = f\"Você é o Assistente {friend_name}, tendo uma conversa amigável e calorosa em português. Responda em português usando linguagem conversacional, expresse empatia e ocasionalmente use frases amigáveis ou humor leve quando apropriado.\"\n",
        "    elif response_language == \"de\":\n",
        "        system_instruction = f\"Sie sind Assistent {friend_name}, führen ein freundliches, herzliches Gespräch auf Deutsch. Antworten Sie auf Deutsch in einer Unterhaltungssprache, zeigen Sie Empathie und verwenden Sie gelegentlich freundliche Ausdrücke oder leichten Humor, wenn es angebracht ist.\"\n",
        "    else:\n",
        "        # Default to English\n",
        "        system_instruction = f\"You are Assistant {friend_name}, having a friendly, warm conversation in English. Respond in English using conversational language, express empathy, and occasionally use friendly phrases or light humor when appropriate.\"\n",
        "\n",
        "    # Format the conversation history with appropriate name\n",
        "    conversation = format_chat_history(history, response_language)\n",
        "\n",
        "    # Add the current user input with the system instruction and friendly framing\n",
        "    prompt = f\"{system_instruction}\\n\\n{conversation}User: {user_input}\\nAssistant {friend_name}:\"\n",
        "\n",
        "    # Generate response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Set stop sequences - update to include assistant's name\n",
        "    stop_sequences = [\"User:\", \"\\nUser\", f\"Assistant {friend_name}:\"]\n",
        "    stop_token_ids = [tokenizer.encode(seq, add_special_tokens=False)[0] for seq in stop_sequences]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.75,\n",
        "            top_p=0.92,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=stop_token_ids\n",
        "        )\n",
        "\n",
        "    # Decode the response and clean it up\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Additional cleaning to ensure we don't have any remaining stop sequences\n",
        "    for stop_seq in stop_sequences:\n",
        "        if stop_seq in response:\n",
        "            response = response.split(stop_seq)[0].strip()\n",
        "\n",
        "    # Verify response language matches expected language\n",
        "    check_response_language = detect_language_with_deepseek(response)\n",
        "    if check_response_language != response_language:\n",
        "        print(f\"Warning: Response language ({check_response_language}) doesn't match expected language ({response_language})\")\n",
        "\n",
        "    return response\n",
        "\n",
        "def format_chat_history(messages, language=None):\n",
        "    \"\"\"Format the chat history for DeepSeek model input with the appropriate friend name\"\"\"\n",
        "    global current_language\n",
        "\n",
        "    # If language is not provided, use the current language\n",
        "    if language is None:\n",
        "        language = current_language\n",
        "\n",
        "    # Get the appropriate friend name\n",
        "    friend_name = get_friend_name(language)\n",
        "\n",
        "    formatted_prompt = \"\"\n",
        "    for user_msg, friend_msg in messages:\n",
        "        formatted_prompt += f\"User: {user_msg}\\nAssistant {friend_name}: {friend_msg}\\n\\n\"\n",
        "    return formatted_prompt\n",
        "\n",
        "def save_voice_sample(audio_file):\n",
        "    \"\"\"Save the user's voice sample to Google Drive\"\"\"\n",
        "    global user_voice_sample\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    sample_filename = f\"voice_sample_{timestamp}.wav\"\n",
        "    drive_path = os.path.join(voice_samples_dir, sample_filename)\n",
        "\n",
        "    # Copy the file to Google Drive\n",
        "    import shutil\n",
        "    shutil.copy(audio_file, drive_path)\n",
        "\n",
        "    user_voice_sample = drive_path\n",
        "\n",
        "    print(f\"Saved voice sample to: {drive_path}\")\n",
        "    return drive_path\n",
        "\n",
        "def set_voice_sample(audio_file):\n",
        "    \"\"\"Save the user's voice sample for cloning\"\"\"\n",
        "    if not audio_file:\n",
        "        return \"Please record or upload a voice sample first.\"\n",
        "\n",
        "    drive_path = save_voice_sample(audio_file)\n",
        "    return f\"Voice sample saved to Google Drive: {drive_path}\"\n",
        "\n",
        "def detect_language_with_deepseek(text):\n",
        "    \"\"\"\n",
        "    Use DeepSeek model to detect language in the user's message\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"en\"  # Default to English for empty text\n",
        "\n",
        "    # Create a simple prompt asking DeepSeek to identify the language\n",
        "    prompt = f\"\"\"Please identify which language this text is written in. Only respond with the language code:\n",
        "- \"en\" for English\n",
        "- \"pt\" for Portuguese\n",
        "- \"de\" for German\n",
        "\n",
        "Text: \"{text}\"\n",
        "\n",
        "Language code:\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=5,  # We only need a short response\n",
        "            temperature=0.1,   # Low temperature for deterministic output\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and clean the response\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    # Extract just the language code\n",
        "    if \"en\" in response:\n",
        "        return \"en\"\n",
        "    elif \"pt\" in response:\n",
        "        return \"pt\"\n",
        "    elif \"de\" in response:\n",
        "        return \"de\"\n",
        "    else:\n",
        "        print(f\"Unexpected language detection response: {response}\")\n",
        "        return \"en\"  # Default to English if we can't parse the response\n",
        "\n",
        "def enhanced_chat_with_text(text, history):\n",
        "    \"\"\"Process text input, detect language, and generate response with TTS\"\"\"\n",
        "    global user_voice_sample, current_language\n",
        "\n",
        "    if not text.strip():\n",
        "        return history, history, \"Please enter a message.\", None\n",
        "\n",
        "    # Detect language of input text using DeepSeek\n",
        "    detected_language = detect_language_with_deepseek(text)\n",
        "\n",
        "    # Always update the language - we trust DeepSeek's detection\n",
        "    previous_language = current_language\n",
        "    current_language = detected_language\n",
        "\n",
        "    language_info = \"\"\n",
        "    if previous_language != current_language:\n",
        "        language_info = f\"Language changed from {current_language_name(previous_language)} to {current_language_name(current_language)}. \"\n",
        "    print(f\"DeepSeek detected language: {current_language}\")\n",
        "\n",
        "    # Get response from DeepSeek\n",
        "    llm_start_time = time.time()\n",
        "    response = generate_response(text, history)\n",
        "    llm_time = time.time() - llm_start_time\n",
        "\n",
        "    # Convert response to speech using language-specific model\n",
        "    speech_file = None\n",
        "    tts_info = f\"LLM: {llm_time:.2f}s | Using language: {current_language}\"\n",
        "\n",
        "    if user_voice_sample is not None:\n",
        "        tts_start_time = time.time()\n",
        "        speech_file = text_to_speech(response, user_voice_sample)\n",
        "        tts_time = time.time() - tts_start_time\n",
        "        tts_info += f\" | TTS: {tts_time:.2f}s\"\n",
        "    else:\n",
        "        tts_info += \" | No voice sample set for TTS\"\n",
        "\n",
        "    # Update history\n",
        "    history.append((text, response))\n",
        "\n",
        "    return history, history, tts_info, speech_file\n",
        "\n",
        "def enhanced_chat_with_voice(audio_file, history):\n",
        "    \"\"\"Process voice input, transcribe, detect language, and generate response with TTS\"\"\"\n",
        "    global user_voice_sample, current_language\n",
        "\n",
        "    if not audio_file:\n",
        "        return history, history, \"No audio detected. Please record your message again.\", None\n",
        "\n",
        "    # Process audio to text\n",
        "    start_time = time.time()\n",
        "    transcription = transcribe_audio(audio_file)\n",
        "    transcription_time = time.time() - start_time\n",
        "\n",
        "    # Detect language from transcription using DeepSeek\n",
        "    previous_language = current_language\n",
        "    detected_language = detect_language_with_deepseek(transcription)\n",
        "    current_language = detected_language\n",
        "\n",
        "    language_info = \"\"\n",
        "    if previous_language != current_language:\n",
        "        language_info = f\"Language changed from {current_language_name(previous_language)} to {current_language_name(current_language)}. \"\n",
        "    print(f\"DeepSeek detected language: {current_language}\")\n",
        "\n",
        "    # If this is the first voice message, use it as voice sample if none exists\n",
        "    if user_voice_sample is None:\n",
        "        drive_path = save_voice_sample(audio_file)\n",
        "        voice_sample_message = f\"Voice sample automatically set from your first message and saved to: {drive_path}\"\n",
        "    else:\n",
        "        voice_sample_message = \"\"\n",
        "\n",
        "    # Get response from DeepSeek\n",
        "    llm_start_time = time.time()\n",
        "    response = generate_response(transcription, history)\n",
        "    llm_time = time.time() - llm_start_time\n",
        "\n",
        "    # Convert response to speech using language-specific model\n",
        "    tts_start_time = time.time()\n",
        "    speech_file = text_to_speech(response, user_voice_sample)\n",
        "    tts_time = time.time() - tts_start_time\n",
        "\n",
        "    # Update history\n",
        "    history.append((transcription, response))\n",
        "\n",
        "    status = f\"Transcription: {transcription_time:.2f}s | LLM: {llm_time:.2f}s | TTS: {tts_time:.2f}s | Language: {current_language}\"\n",
        "    if voice_sample_message:\n",
        "        status += f\"\\n{voice_sample_message}\"\n",
        "\n",
        "    return history, history, status, speech_file\n",
        "\n",
        "def set_language(lang_code):\n",
        "    \"\"\"Manually set the language\"\"\"\n",
        "    global current_language\n",
        "\n",
        "    if lang_code in [\"en\", \"pt\", \"de\"]:\n",
        "        current_language = lang_code\n",
        "        return f\"Language manually set to: {current_language_name(lang_code)} ({lang_code})\"\n",
        "    else:\n",
        "        return f\"Invalid language code: {lang_code}. Supported codes: en, pt, de\"\n",
        "\n",
        "def current_language_name(lang_code):\n",
        "    \"\"\"Get the full name of a language from its code\"\"\"\n",
        "    names = {\"en\": \"English\", \"pt\": \"Portuguese\", \"de\": \"German\"}\n",
        "    return names.get(lang_code, \"Unknown\")\n",
        "\n",
        "def save_conversation(history):\n",
        "    \"\"\"Save the current conversation to a text file on Google Drive\"\"\"\n",
        "    global current_language\n",
        "\n",
        "    if not history:\n",
        "        return \"No conversation to save.\"\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"conversation_{timestamp}.txt\"\n",
        "    filepath = os.path.join(conversations_dir, filename)\n",
        "\n",
        "    # Get the friend name based on the current language\n",
        "    friend_name = get_friend_name(current_language)\n",
        "\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Conversation with Assistant {friend_name} ({current_language_name(current_language)})\\n\\n\")\n",
        "        for i, (user_msg, friend_msg) in enumerate(history):\n",
        "            f.write(f\"Turn {i+1}:\\n\")\n",
        "            f.write(f\"User: {user_msg}\\n\\n\")\n",
        "            f.write(f\"Assistant {friend_name}: {friend_msg}\\n\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "    return f\"Conversation with Assistant {friend_name} saved to Google Drive: {filepath}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Multilingual Voice Chat with Language-Specific Models\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            # Display chat history\n",
        "            chatbot = gr.Chatbot(label=\"Chat History\")\n",
        "\n",
        "            # Audio output for TTS responses\n",
        "            audio_output = gr.Audio(label=\"AI Voice Response\", autoplay=True)\n",
        "\n",
        "            # Status message and current language\n",
        "            status_msg = gr.Markdown(\"\")\n",
        "            current_lang_indicator = gr.Markdown(f\"**Current Language**: English (en) | **Assistant**: {get_friend_name('en')}\")\n",
        "\n",
        "            # Text input option\n",
        "            with gr.Row():\n",
        "                text_input = gr.Textbox(placeholder=\"Type your message here...\", label=\"Text Input\")\n",
        "                text_submit = gr.Button(\"Send Text\")\n",
        "\n",
        "            # Audio input option\n",
        "            with gr.Row():\n",
        "                audio_input = gr.Audio(\n",
        "                    label=\"Voice Input\",\n",
        "                    type=\"filepath\",\n",
        "                    sources=[\"microphone\", \"upload\"]\n",
        "                )\n",
        "                audio_submit = gr.Button(\"Send Voice\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Voice Cloning Settings\")\n",
        "            voice_sample_input = gr.Audio(\n",
        "                label=\"Record Voice Sample for Cloning\",\n",
        "                type=\"filepath\",\n",
        "                sources=[\"microphone\", \"upload\"]\n",
        "            )\n",
        "            voice_sample_button = gr.Button(\"Set Voice Sample\")\n",
        "            voice_sample_status = gr.Markdown(\"\")\n",
        "\n",
        "            gr.Markdown(\"## Language Settings\")\n",
        "            with gr.Row():\n",
        "                en_button = gr.Button(\"English\")\n",
        "                pt_button = gr.Button(\"Portuguese\")\n",
        "                de_button = gr.Button(\"German\")\n",
        "            language_override_status = gr.Markdown(\"\")\n",
        "\n",
        "            gr.Markdown(\"## Save Conversation\")\n",
        "            save_conversation_button = gr.Button(\"Save Conversation to Drive\")\n",
        "            save_status = gr.Markdown(\"\")\n",
        "\n",
        "            gr.Markdown(\"## Storage Information\")\n",
        "            gr.Markdown(f\"Voice samples: {voice_samples_dir}\")\n",
        "            gr.Markdown(f\"AI responses: {responses_dir}\")\n",
        "            gr.Markdown(f\"Conversations: {conversations_dir}\")\n",
        "\n",
        "    # Store chat history state\n",
        "    state = gr.State([])\n",
        "\n",
        "    # Set up event handlers for language buttons\n",
        "    def language_button_handler(lang_code):\n",
        "        global current_language\n",
        "        current_language = lang_code\n",
        "        friend_name = get_friend_name(lang_code)\n",
        "        return f\"Language manually set to: {current_language_name(lang_code)} ({lang_code}) | Assistant: {friend_name}\"\n",
        "\n",
        "    en_button.click(\n",
        "        lambda: language_button_handler(\"en\"),\n",
        "        inputs=[],\n",
        "        outputs=[language_override_status]\n",
        "    )\n",
        "\n",
        "    pt_button.click(\n",
        "        lambda: language_button_handler(\"pt\"),\n",
        "        inputs=[],\n",
        "        outputs=[language_override_status]\n",
        "    )\n",
        "\n",
        "    de_button.click(\n",
        "        lambda: language_button_handler(\"de\"),\n",
        "        inputs=[],\n",
        "        outputs=[language_override_status]\n",
        "    )\n",
        "\n",
        "    # Wrap the chat functions to update the language display\n",
        "    def wrapped_chat_with_text(text, history):\n",
        "        history, state, status, speech_file = enhanced_chat_with_text(text, history)\n",
        "        text_input.value = \"\"  # Clear the text input field\n",
        "        friend_name = get_friend_name(current_language)\n",
        "        return history, state, status, speech_file, f\"**Current Language**: {current_language_name(current_language)} ({current_language}) | **Assistant**: {friend_name}\"\n",
        "\n",
        "    def wrapped_chat_with_voice(audio_file, history):\n",
        "        history, state, status, speech_file = enhanced_chat_with_voice(audio_file, history)\n",
        "        friend_name = get_friend_name(current_language)\n",
        "        return history, state, status, speech_file, f\"**Current Language**: {current_language_name(current_language)} ({current_language}) | **Assistant**: {friend_name}\"\n",
        "\n",
        "    # Set up event handlers for chat\n",
        "    text_submit.click(\n",
        "        wrapped_chat_with_text,\n",
        "        inputs=[text_input, state],\n",
        "        outputs=[chatbot, state, status_msg, audio_output, current_lang_indicator]\n",
        "    )\n",
        "\n",
        "    text_input.submit(\n",
        "        wrapped_chat_with_text,\n",
        "        inputs=[text_input, state],\n",
        "        outputs=[chatbot, state, status_msg, audio_output, current_lang_indicator]\n",
        "    )\n",
        "\n",
        "    audio_submit.click(\n",
        "        wrapped_chat_with_voice,\n",
        "        inputs=[audio_input, state],\n",
        "        outputs=[chatbot, state, status_msg, audio_output, current_lang_indicator]\n",
        "    )\n",
        "\n",
        "    # Other event handlers\n",
        "    voice_sample_button.click(\n",
        "        set_voice_sample,\n",
        "        inputs=[voice_sample_input],\n",
        "        outputs=[voice_sample_status]\n",
        "    )\n",
        "\n",
        "    save_conversation_button.click(\n",
        "        save_conversation,\n",
        "        inputs=[state],\n",
        "        outputs=[save_status]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "3SMjViKRdH2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}